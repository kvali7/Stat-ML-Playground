{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 208: Homework 3 (Do not distribute)\n",
    "\n",
    "__Instructions:__ Submit it on canvas.  The canvas should include all of your code either in this notebook file, or a separate python file that is imported and ran in this notebook.  We should be able to open this notebook and run everything here by running the cells in sequence.  The written portions can be either done in markdown and TeX in new cells or written clearly by hand when you hand it in.  Submit each file separately.\n",
    "\n",
    "- Code should be well organized and documented\n",
    "- All math should be clear and make sense sequentially\n",
    "- When in doubt explain what is going on\n",
    "- You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__ (10 pts)\n",
    "\n",
    "Recall that surrogate losses for large margin classification take the form, $\\phi(y_i x_i^\\top \\beta)$ where $y_i \\in \\{-1,1\\}$ and $\\beta, x_i \\in \\mathbb R^p$.\n",
    "\n",
    "The following functions are used as surrogate losses for large margin classification.  Demonstrate if they are convex or not, and follow the instructions.\n",
    "\n",
    "1. exponential loss: $\\phi(x) = e^{-x}$\n",
    "1. truncated quadratic loss: $\\phi(x) = (\\max\\{1-x,0\\})^2$\n",
    "1. hinge loss: $\\phi(x) = \\max\\{1-x,0\\}$\n",
    "1. sigmoid loss: $\\phi(x) = 1 - \\tanh(\\kappa x)$, for fixed $\\kappa > 0$\n",
    "1. Plot these as a function of $x$.\n",
    "\n",
    "(This problem is due to notes of Larry Wasserman.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__ (20 pts)\n",
    "\n",
    "Consider the truncated quadratic loss from (1.1.2).  For brevity let $a_+ = max\\{a,0\\}$ denote the positive part of $a$.\n",
    "\n",
    "$$\\ell(y_i,x_i,\\beta) = \\phi(y_i x_i^\\top \\beta) = (1-y_i x_i^\\top \\beta)_+^2$$\n",
    "\n",
    "1. Consider the empirical risk, $R_n$ (the average loss over a training set) for the truncated quadratic loss.  What is gradient of $R_n$ in $\\beta$?  Does it always exists?\n",
    "1. Demonstrate that the gradient does not have continuous derivative everywhere.\n",
    "1. Recall that support vector machines used the hinge loss $(1 - y_i x_i^\\top \\beta)_+$ with a ridge regularization.  Write the regularized optimization method for the truncated quadratic loss, and derive the gradient of the regularized empirical risk.\n",
    "1. In quasi-Newton methods a matrix ($Q$) that is a surrogate for the Hessian of the objective $L$ is used to determine step direction.\n",
    "\n",
    "$$\n",
    "\\beta \\gets \\beta - Q^{-1} \\nabla L(\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "Because the loss does not have continuous Hessian, instead of the Newton method, we will use a quasi-Newton method that replaces the Hessian with a quasi-Hessian (another matrix that is meant to approximate the Hessian).  Consider the following quasi-Hessian of the regularized objective to be $$G(\\beta) = \\frac 1n \\sum_i 2 (x_i x_i^\\top 1\\{ y_i x_i^\\top \\beta < 1 \\}) + 2 \\lambda I.$$  Demonstrate that the quasi-Hessian is positive definite, and write pseudo-code for quasi-Newton optimization, comment on the computational complexity of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 (20 pts)**\n",
    "\n",
    "Consider the simulation below.\n",
    "\n",
    "1. Implement minibatch stochastic gradient descent using the truncated quadratic loss.  Access the data by iteratively calling the ``sim_data`` method below.  \n",
    "\n",
    "2. With minibatch size of $1$ (SGD).  Vary to learning schedule to be constant, decaying with $\\eta_t \\propto t**{-1/2}$, and $\\eta_t \\propto t^{-1}$.  Compare with normal noise (the ``noise_dis`` parm).\n",
    "\n",
    "3. Vary the minibatch size to see the change in performance, with the best learning schedule from 2. When you compare two methods, make sure that you compare them with the same amount of data accessed (so use 1:10 ratio of iterations if you are comparing a minibatch ratio of 10:1).\n",
    "\n",
    "4. Redo 2, 3 with ``noise_dis`` set to ``\"chisquare\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSimulator:\n",
    "    \"\"\"\n",
    "    Simulate the data for linear classification\n",
    "    \"\"\"\n",
    "    def __init__(self,p,noise_dist = \"normal\"):\n",
    "        self.beta = np.random.normal(0,1,p)\n",
    "        self.noise_dist = noise_dist\n",
    "        self.p = p\n",
    "        \n",
    "    def sim_data(self,m = 1):\n",
    "        p = self.p\n",
    "        X = np.random.normal(0,1,(m,p))\n",
    "        if self.noise_dist == \"normal\":\n",
    "            eps = np.random.normal(0,1,m)\n",
    "        if self.noise_dist == \"chisquare\":\n",
    "            eps = np.random.chisquare(1,m)\n",
    "        z = X @ self.beta + eps\n",
    "        y = 1*(z > 0)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-8.82760716e-01,  3.11498912e-01,  9.23795147e-01,\n",
       "          1.61536818e+00,  8.96260892e-01,  3.97902536e-01,\n",
       "         -7.26042790e-01, -8.15414265e-02,  2.40711842e-01,\n",
       "          1.54514937e+00],\n",
       "        [-9.09026409e-01,  1.84392586e-01,  7.11932756e-01,\n",
       "          4.58398533e-01, -2.68785064e+00,  4.01885117e-01,\n",
       "         -2.08945052e-01, -3.85312070e-04, -8.22808981e-01,\n",
       "         -1.28058200e+00],\n",
       "        [ 1.09881842e+00,  8.59804799e-01, -4.74453657e-01,\n",
       "          3.58426865e-01,  9.96840138e-01,  1.36254269e+00,\n",
       "          5.00507551e-01, -3.15185392e-01,  1.46405671e-01,\n",
       "          7.70887297e-01],\n",
       "        [-4.39329053e-01,  4.72952643e-02, -1.75216771e+00,\n",
       "         -6.00553048e-01,  1.87289087e-01, -5.59439958e-01,\n",
       "         -6.25566297e-01,  1.33319438e-01, -1.05928270e+00,\n",
       "          5.15527106e-01],\n",
       "        [ 7.92426056e-01, -5.14292065e-01, -3.97204403e-01,\n",
       "         -4.49388093e-02,  1.34709454e-01,  4.26510677e-01,\n",
       "          1.39772818e-01, -2.67506249e-01,  7.85905597e-01,\n",
       "         -7.85928979e-01],\n",
       "        [-3.73756959e-01, -1.67520892e+00, -7.55010658e-01,\n",
       "          3.80090338e-01, -3.76949602e-01,  5.13669949e-01,\n",
       "         -2.24002334e-01, -1.79050909e+00,  1.19470174e+00,\n",
       "         -8.41498934e-01],\n",
       "        [ 8.62779210e-02, -6.53998619e-01, -2.31624731e-01,\n",
       "          1.02848711e+00, -1.33877317e+00, -1.65369492e+00,\n",
       "         -6.98819478e-01, -1.28458941e+00, -1.45591577e+00,\n",
       "          1.47157419e+00],\n",
       "        [-1.28265459e+00, -1.33089387e+00, -8.60199337e-01,\n",
       "         -1.89336134e+00,  5.57891072e-01,  1.59841482e+00,\n",
       "         -1.38349288e+00,  7.31398594e-01,  2.29028843e-01,\n",
       "         -2.33605868e+00],\n",
       "        [-2.51178688e+00, -1.05153981e+00, -8.47615098e-01,\n",
       "          1.00070004e+00, -2.28306575e-02,  1.39963866e+00,\n",
       "          1.21992518e+00, -7.26248622e-01, -6.67474434e-01,\n",
       "          9.52375117e-02],\n",
       "        [ 2.78709062e+00, -1.55672483e+00,  1.83076901e+00,\n",
       "          2.32674656e-01,  1.52503766e+00, -1.08504137e+00,\n",
       "          1.13583270e+00, -2.92048525e+00, -4.79237521e-02,\n",
       "          2.07187843e+00]]), array([1, 0, 1, 0, 1, 1, 0, 0, 0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DataSimulator(10)\n",
    "ds.sim_datum(m=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.__ (50 pts) \n",
    "\n",
    "Text data can be converted into vector data through a vectorization operation.  A corpus is a collection of documents and the dictionary is all of the words in the corpus.  Bag-of-words models will treat each document as a set of words, ignoring the order of the words.  Then a simple vectorizer will let $X_{i,j}$ be the number of times the $j$th word is in the $i$th document.  Two vectorizers are ``sklearn.feature_extraction.text.CountVectorizer`` and ``sklearn.feature_extraction.text.TfidfVectorizer``.\n",
    "\n",
    "Below is an import of a reuters dataset.  I have written a def to process a single file.  Construct a response variable that has three categories, if the topic is 'earn', 'acq', or another category.  Import all of the data and construct two sparse vectorized matrices---look at ``scipy.sparse``---based on the two above vectorizations.  Use sklearn svm.SVC on the TRAIN split and predict on the TEST split.  Plot your ROC and PR curves for predicting 'earn' (versus everything else); tune the kernel and C parameters.  Do the same for predicting 'acq' versus everything else.  Write a paragraph summarizing the performance and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html, etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reu = html.parse(\"reuters/reut2-000.sgm\") #You will have to do this for all sgm files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "# Download Corpora -> stopwords, Models -> punkt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reu(reu):\n",
    "    \"\"\"Parses the etree object and returns a list of dictionary of reuters attr\n",
    "    Output: {'topics': the topic of the article, 'places': where it is located, \n",
    "        'split': training/test split, 'body':the text of the article as a set of words with stopwords removed}\n",
    "    \"\"\"\n",
    "    root= reu.getroot()\n",
    "    articles = root.body.getchildren()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    reu_pl = []\n",
    "    for a in articles:\n",
    "        reu_parse = {}\n",
    "        if a.attrib['topics'] != 'YES':\n",
    "            next\n",
    "        topics = a.find('topics').findall('d')\n",
    "        if topics:\n",
    "            reu_parse['topics'] = [t.text for t in topics]\n",
    "        else:\n",
    "            reu_parse['topics'] = []\n",
    "        places = a.find('places').findall('d')\n",
    "        if places:\n",
    "            reu_parse['places'] = [t.text for t in places]\n",
    "        reu_parse['split'] = a.attrib['lewissplit']\n",
    "        rtxt = a.find('text')\n",
    "        word_tokens = word_tokenize(rtxt.text_content())\n",
    "        filtered_sentence = set([w.lower() for w in word_tokens if not w in stop_words])\n",
    "        reu_parse['body'] = filtered_sentence\n",
    "        reu_pl.append(reu_parse)\n",
    "    return reu_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_pl = parse_reu(reu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cocoa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'review sept lower 60 155,221 bean 4,350 week in may still held 5.81 kilos harvesting expected convertible 1986/87 much destinations making cumulative showers farmers 340 per u.s. prospects mln 2,400 27 july june/july routine comissaria delivered registered 4,450 final bags 1,780 dec crop early hundred april/may rose uruguay temporao 350 coming improving 1,750 785 thousand included fit oct/dec went aug butter late available reuter currency 6.13 ends part booked seems 1987/88 quality dificulties 753 export old february superior+ the 1,875 +bahia drought weeks consignment 4,400 view again 35 2,375 last - 1.25 ports selling 22 experiencing . commission middlemen march 1,880 feb tonne 4,351 currently times 6.2 levels throughout come arroba light , practically shipment 28 doubts continued recent new with there 2.28 restored trade standing would 4,340 sales prices york period since dlrs 2,380 doubt salvador zone nearby 4,415 january earlier 2,325 1.06 26 named cake going said 15 buyers sold 6.4 processors published around good means dry almost humidity figures limited liquor normal bahia cruzados 1,850 fob march/april areas arrivals spot ended 0.39 5.93 certificates 1,870 aug/sept smith season 2.27 estimates offer total 4,345 estimated 45 midday exporters although cocoa also made argentina alleviating carnival shippers end weekly obtaining covertible reluctant hands stage open 4,480 brazilian 995 year'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reu_pl[0]['topics'])\n",
    "\" \".join(reu_pl[0]['body'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
