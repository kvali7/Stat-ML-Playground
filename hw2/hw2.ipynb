{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 208: Homework 2 (Do not distribute)\n",
    "\n",
    "__Instructions:__ To make grading easier for the TAs please submit it on canvas in a zip file with either the scanned solutions clearly written or in the jupyter notebook below.  The zip file should include all of your code either in this notebook file, or a separate python file that is imported and ran in this notebook.  We should be able to open this notebook and run everything here by running the cells in sequence.  The written portions can be either done in markdown and TeX in new cells or written clearly by hand and scanned.\n",
    "\n",
    "- Code should be well organized and documented\n",
    "- All math should be clear and make sense sequentially\n",
    "- When in doubt explain what is going on\n",
    "- You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__ (30 pts)\n",
    "\n",
    "Consider Poisson model with rate parameter $\\lambda$ which has PMF,\n",
    "$$\n",
    "p(y|\\lambda) = \\frac{\\lambda^y}{y!} e^{-\\lambda},\n",
    "$$\n",
    "where $y = 0,1,\\ldots$ is some count variable.\n",
    "In Poison regression, we model $\\lambda = e^{\\beta^\\top x}$ to obtain $p(y | x,\\beta)$.\n",
    "\n",
    "1. Let the loss function for Poisson regression be $\\ell_i(\\beta) \\propto - \\log p(y_i | x_i, \\beta)$ for a dataset consisting of predictor variables and count values $\\{x_i,y_i\\}_{i=1}^n$.  Here $\\propto$ means that we disregard any additive terms that are not dependent on $\\beta$.  Write an expression for $\\ell_i$ and derive its gradient. \n",
    "2. Show that the empirical risk $R_n(\\beta)$ is a convex function of $\\beta$.\n",
    "3. Consider the mapping $F_\\eta(\\beta) = \\beta - \\eta \\nabla R_n(\\beta)$ which is the iteration of gradient descent ($\\eta>0$ is called the learning parameter).  Show that at the minimizer of $R_n$, $\\hat \\beta$, we have that $F(\\hat \\beta) = \\hat \\beta$.\n",
    "4. I have a script to simulate from this model below.  Implement the gradient descent algorithm above and show that with enough data (n large enough) the estimated $\\hat \\beta$ approaches the true $\\beta$ (you can look at the sum of square error between these two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulate from the Poisson regression model (use y,X)\n",
    "n, p = 1000,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "lamb = np.exp(X @ beta)\n",
    "y = np.random.poisson(lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__ (30 pts)\n",
    "\n",
    "Recall the subset selection problem with tuning parameter $k$,\n",
    "$$\n",
    "\\min_{\\beta : |{\\rm supp}(\\beta)| \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where \n",
    "${\\rm supp}(\\beta) = \\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$. Notice that we can write this as \n",
    "$$\n",
    "\\min_{\\beta : \\| \\beta \\|_0 \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where \n",
    "1. Write the subset selection problem in the following form\n",
    "$$\n",
    "\\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top P_S y,\n",
    "$$\n",
    "where $P_S$ is a projection.  Describe the subspace that is the range of this projection.\n",
    "2. Suppose that we have a nested sequence of models $S_1\\subset S_2 \\subset \\ldots \\subset S_p$ such that $|S_k| = k$ (it contains $k$ variables).  Prove that $$y^\\top P_{S_k} y \\ge y^\\top P_{S_{k+1}} y$$ for $k=1,\\ldots,p-1$.  What does this tell us about the solution to the subset selection problem and the constraint $|S| \\le k$?\n",
    "3. Suppose that $X$ is orthogonal, then write a computationally efficient pseudocode to solve the subset selection problem.  Prove that it is correct (your algorithm actually solves subset selection under othogonal design).\n",
    "4. (Challenge) Suppose that we have that $n = p$ and $y_i = \\beta_i + \\epsilon_i$ (identity design matrix) where $\\epsilon_i$ satisfies \n",
    "$$\n",
    "\\mathbb P \\left\\{ |\\epsilon_i| \\ge t \\right\\} \\le 2 e^{-t^2 / 2\\sigma^2}\n",
    "$$\n",
    "for any $t > 0$ (this is true for central Normal RVs) for some $\\sigma > 0$.\n",
    "Suppose that there is some true $S_0 \\subset\\{1,\\ldots,p\\}$ such that $|S_0| = k < p$ and ${\\rm supp}(\\beta) = S_0$.\n",
    "Prove the following.\n",
    "\n",
    "__Proposition__\n",
    "Define $\\mu = \\min_{j \\in S_0} |\\beta_j|$ and call $\\mu / \\sigma$ the signal-to-noise ratio.  Then if \n",
    "$$\n",
    "\\frac{\\mu}{\\sigma} > 2 \\sqrt{2 \\log \\left( \\frac{2n}{\\delta}\\right)},\n",
    "$$\n",
    "then the true $S$ is selected by subset selection with probability at least $1 - \\delta$.\n",
    "\n",
    "Hint: rewrite the subset selection problem with $X = I$ and compare the objective at $S_0$ to any other $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__ (40 pts)\n",
    "For this exercise, it may be helpful to use the `sklearn.linear_model` module.  I have also included a plotting tool for making the lasso path in ESL.\n",
    "\n",
    "1. Load the training and test data using the script below.  Fit OLS on the training dataset and compute the test error.  Throughout you do not need to compute an intercept but you should normalize the X (divide by the column norms).\n",
    "2. Train and tune ridge regression using a validation set and compute the test error (square error loss).\n",
    "3. Fit the lasso path with lars to the data and compute the test error for each returned lasso coefficient.\n",
    "4. Perform 3 without the lasso modification generating the lars path.  Compare and contrast the lars path to the lasso path, what is the key difference.  Tell me when the active sets differ and how, if they do at all.\n",
    "4. Extract each active set from the lasso path and recompute the restricted OLS for each.  Compute and compare the test error for each model.\n",
    "5. If your boss said that they wanted a more parsimonious model (a smaller model) then which model would you choose, justify your answer.  Under which circumstance would you choose the model with the smallest test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_lars(coefs, lines=False, title=\"Lars Path\"):\n",
    "    \"\"\"\n",
    "    Plot the lasso path where coefs is a matrix - the columns are beta vectors\n",
    "    \"\"\"\n",
    "    xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "    xx /= xx[-1]\n",
    "    plt.plot(xx, coefs.T)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    if lines:\n",
    "        plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "    plt.xlabel('|coef| / max|coef|')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title(title)\n",
    "    plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hw2.data','rb') as f:\n",
    "    y_tr,X_tr,y_te,X_te = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
